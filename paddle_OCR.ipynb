{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import json\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# def create_training_list(dataset_path, output_path, split='train'):\n",
    "#     \"\"\"\n",
    "#     Create training list file in PaddleOCR format\n",
    "#     Format: image_path\\tlabel\n",
    "#     \"\"\"\n",
    "#     data_dir = Path(dataset_path) / split\n",
    "#     output_file = Path(output_path) / f'{split}_list.txt'\n",
    "    \n",
    "#     with open(output_file, 'w', encoding='utf-8') as f:\n",
    "#         for img_path in data_dir.glob('*.*'):\n",
    "#             if img_path.suffix.lower() in ['.png', '.jpg', '.jpeg']:\n",
    "#                 # Get label from filename\n",
    "#                 label = img_path.stem\n",
    "#                 # Write in PaddleOCR format\n",
    "#                 f.write(f'{str(img_path)}\\t{label}\\n')\n",
    "    \n",
    "#     print(f\"Created {split} list at: {output_file}\")\n",
    "def create_training_list(dataset_path, output_path, split='train'):\n",
    "    \"\"\"\n",
    "    Create training list file in PaddleOCR format\n",
    "    Format: image_path\\tlabel\n",
    "    \"\"\"\n",
    "    data_dir = Path(dataset_path) / split\n",
    "    output_file = Path(output_path) / f'{split}_list.txt'\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for img_path in data_dir.glob('*.*'):\n",
    "            if img_path.suffix.lower() in ['.png', '.jpg', '.jpeg']:\n",
    "                # Get label from filename\n",
    "                label = img_path.stem\n",
    "                # Write in PaddleOCR format\n",
    "                f.write(f'{str(img_path)}\\t{label}\\n')\n",
    "    \n",
    "    print(f\"Created {split} list at: {output_file}\")\n",
    "\n",
    "def create_dict_file(dataset_path, output_path):\n",
    "    \"\"\"\n",
    "    Create dictionary file containing all characters in the dataset\n",
    "    \"\"\"\n",
    "    chars = set()\n",
    "    \n",
    "    # Collect all unique characters from all splits\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        data_dir = Path(dataset_path) / split\n",
    "        for img_path in data_dir.glob('*.*'):\n",
    "            if img_path.suffix.lower() in ['.png', '.jpg', '.jpeg']:\n",
    "                chars.update(img_path.stem)\n",
    "    \n",
    "    # Sort and write to file\n",
    "    chars = sorted(list(chars))\n",
    "    dict_file = Path(output_path) / 'dict.txt'\n",
    "    \n",
    "    with open(dict_file, 'w', encoding='utf-8') as f:\n",
    "        for char in chars:\n",
    "            f.write(f'{char}\\n')\n",
    "    \n",
    "    print(f\"Created dictionary file at: {dict_file}\")\n",
    "    return len(chars)\n",
    "\n",
    "def create_config(output_path, num_classes):\n",
    "    \"\"\"\n",
    "    Create PaddleOCR training configuration\n",
    "    \"\"\"\n",
    "    config = {\n",
    "        \"Architecture\": {\n",
    "            \"model_type\": \"rec\",\n",
    "            \"algorithm\": \"CRNN\",\n",
    "            \"Transform\": None,\n",
    "            \"Backbone\": {\n",
    "                \"name\": \"ResNet\",\n",
    "                \"layers\": 34\n",
    "            },\n",
    "            \"Neck\": {\n",
    "                \"name\": \"SequenceEncoder\",\n",
    "                \"encoder_type\": \"rnn\",\n",
    "                \"hidden_size\": 256\n",
    "            },\n",
    "            \"Head\": {\n",
    "                \"name\": \"CTCHead\",\n",
    "                \"fc_decay\": 0.0004\n",
    "            }\n",
    "        },\n",
    "        \"Loss\": {\n",
    "            \"name\": \"CTCLoss\"\n",
    "        },\n",
    "        \"Optimizer\": {\n",
    "            \"name\": \"Adam\",\n",
    "            \"beta1\": 0.9,\n",
    "            \"beta2\": 0.999,\n",
    "            \"lr\": {\n",
    "                \"name\": \"Cosine\",\n",
    "                \"learning_rate\": 0.001,\n",
    "                \"warmup_epoch\": 5\n",
    "            }\n",
    "        },\n",
    "        \"Train\": {\n",
    "            \"dataset\": {\n",
    "                \"name\": \"SimpleDataSet\",\n",
    "                \"data_dir\": \"dataset/\",\n",
    "                \"label_file_list\": [\"train_list.txt\"],\n",
    "                \"transforms\": [\n",
    "                    {\"DecodeImage\": {\"img_mode\": \"RGB\", \"channel_first\": False}},\n",
    "                    {\"RecResizeImg\": {\"image_shape\": [3, 32, 100]}},\n",
    "                    {\"KeepKeys\": {\"keep_keys\": [\"image\", \"label\"]}}\n",
    "                ]\n",
    "            },\n",
    "            \"loader\": {\n",
    "                \"batch_size_per_card\": 32,\n",
    "                \"drop_last\": True,\n",
    "                \"num_workers\": 4,\n",
    "                \"shuffle\": True,\n",
    "            }\n",
    "        },\n",
    "        \"Eval\": {\n",
    "            \"dataset\": {\n",
    "                \"name\": \"SimpleDataSet\",\n",
    "                \"data_dir\": \"dataset/\",\n",
    "                \"label_file_list\": [\"val_list.txt\"],\n",
    "                \"transforms\": [\n",
    "                    {\"DecodeImage\": {\"img_mode\": \"RGB\", \"channel_first\": False}},\n",
    "                    {\"RecResizeImg\": {\"image_shape\": [3, 32, 100]}},\n",
    "                    {\"KeepKeys\": {\"keep_keys\": [\"image\", \"label\"]}}\n",
    "                ]\n",
    "            },\n",
    "            \"loader\": {\n",
    "                \"batch_size_per_card\": 32,\n",
    "                \"drop_last\": False,\n",
    "                \"num_workers\": 4,\n",
    "                \"shuffle\": False,\n",
    "            }\n",
    "        },\n",
    "        \"Global\": {\n",
    "            \"epoch_num\": 100,\n",
    "            \"save_model_dir\": \"./output/rec_crnn\",\n",
    "            \"save_epoch_step\": 5,\n",
    "            \"eval_batch_step\": [0, 2000],\n",
    "            \"cal_metric_during_train\": True,\n",
    "            \"pretrained_model\": \"./pretrain_models/CRNN/best_accuracy\",\n",
    "            \"checkpoints\": None,\n",
    "            \"save_inference_dir\": None,\n",
    "            \"use_visualdl\": True,\n",
    "            \"class_num\": num_classes + 1,  # Add 1 for blank token\n",
    "            \"character_dict_path\": \"dict.txt\",\n",
    "            \"character_type\": \"ch\",\n",
    "            \"use_space_char\": False\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    config_file = Path(output_path) / 'config.yml'\n",
    "    with open(config_file, 'w') as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "    \n",
    "    print(f\"Created configuration file at: {config_file}\")\n",
    "\n",
    "def prepare_training():\n",
    "    \"\"\"\n",
    "    Prepare all necessary files for PaddleOCR training\n",
    "    \"\"\"\n",
    "    # Create output directory\n",
    "    output_path = Path('paddleocr_training')\n",
    "    output_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Create training lists\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        create_training_list('dataset', output_path, split)\n",
    "    \n",
    "    # Create dictionary\n",
    "    num_classes = create_dict_file('dataset', output_path)\n",
    "    \n",
    "    # Create configuration\n",
    "    create_config(output_path, num_classes)\n",
    "    \n",
    "    print(\"\\nTraining preparation completed!\")\n",
    "    print(\"\\nTo start training, follow these steps:\")\n",
    "    print(\"1. Install PaddlePaddle and PaddleOCR:\")\n",
    "    print(\"   pip install paddlepaddle-gpu paddleocr\")\n",
    "    print(\"\\n2. Run training:\")\n",
    "    print(\"   python PaddleOCR/tools/train.py -c paddleocr_training/config.yml\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     prepare_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created train list at: paddleocr_training\\train_list.txt\n",
      "Created val list at: paddleocr_training\\val_list.txt\n",
      "Created test list at: paddleocr_training\\test_list.txt\n",
      "Created dictionary file at: paddleocr_training\\dict.txt\n",
      "Created configuration file at: paddleocr_training\\config.yml\n",
      "\n",
      "Training preparation completed!\n",
      "\n",
      "To start training, follow these steps:\n",
      "1. Install PaddlePaddle and PaddleOCR:\n",
      "   pip install paddlepaddle-gpu paddleocr\n",
      "\n",
      "2. Run training:\n",
      "   python PaddleOCR/tools/train.py -c paddleocr_training/config.yml\n"
     ]
    }
   ],
   "source": [
    "prepare_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_training_list(dataset_path, output_path, split='train'):\n",
    "#     \"\"\"\n",
    "#     Create training list file in PaddleOCR format\n",
    "#     Format: image_path\\tlabel\n",
    "#     \"\"\"\n",
    "#     data_dir = Path(dataset_path) / split\n",
    "#     output_file = Path(output_path) / f'{split}_list.txt'\n",
    "    \n",
    "#     with open(output_file, 'w', encoding='utf-8') as f:\n",
    "#         for img_path in data_dir.glob('*.*'):\n",
    "#             if img_path.suffix.lower() in ['.png', '.jpg', '.jpeg']:\n",
    "#                 # Get label from filename\n",
    "#                 label = img_path.stem\n",
    "#                 label = re.sub(r'_.*', '', label)\n",
    "#                 # Write in PaddleOCR format\n",
    "#                 rel_path = img_path.relative_to(Path(dataset_path))\n",
    "#                 f.write(f'{rel_path}\\t{label}\\n')\n",
    "    \n",
    "#     print(f\"Created {split} list at: {output_file}\")\n",
    "\n",
    "# def create_dict_file(dataset_path, output_path):\n",
    "#     \"\"\"\n",
    "#     Create dictionary file containing all characters in the dataset\n",
    "#     \"\"\"\n",
    "#     chars = set()\n",
    "    \n",
    "#     # Collect all unique characters from all splits\n",
    "#     for split in ['train', 'val', 'test']:\n",
    "#         data_dir = Path(dataset_path) / split\n",
    "#         for img_path in data_dir.glob('*.*'):\n",
    "#             if img_path.suffix.lower() in ['.png', '.jpg', '.jpeg']:\n",
    "#                 chars.update(img_path.stem)\n",
    "    \n",
    "#     # Sort and write to file\n",
    "#     chars = sorted(list(chars))\n",
    "#     dict_file = Path(output_path) / 'dict.txt'\n",
    "    \n",
    "#     with open(dict_file, 'w', encoding='utf-8') as f:\n",
    "#         for char in chars:\n",
    "#             f.write(f'{char}\\n')\n",
    "    \n",
    "#     print(f\"Created dictionary file at: {dict_file}\")\n",
    "#     return len(chars)\n",
    "\n",
    "# def create_config(output_path, num_classes):\n",
    "#     \"\"\"\n",
    "#     Create PaddleOCR training configuration\n",
    "#     \"\"\"\n",
    "#     config = {\n",
    "#         \"Architecture\": {\n",
    "#             \"model_type\": \"rec\",\n",
    "#             \"algorithm\": \"CRNN\",\n",
    "#             \"Transform\": None,\n",
    "#             \"Backbone\": {\n",
    "#                 \"name\": \"ResNet\",\n",
    "#                 \"layers\": 34\n",
    "#             },\n",
    "#             \"Neck\": {\n",
    "#                 \"name\": \"SequenceEncoder\",\n",
    "#                 \"encoder_type\": \"rnn\",\n",
    "#                 \"hidden_size\": 256\n",
    "#             },\n",
    "#             \"Head\": {\n",
    "#                 \"name\": \"CTCHead\",\n",
    "#                 \"fc_decay\": 0.0004\n",
    "#             }\n",
    "#         },\n",
    "#         \"Loss\": {\n",
    "#             \"name\": \"CTCLoss\"\n",
    "#         },\n",
    "#         \"Optimizer\": {\n",
    "#             \"name\": \"Adam\",\n",
    "#             \"beta1\": 0.9,\n",
    "#             \"beta2\": 0.999,\n",
    "#             \"lr\": {\n",
    "#                 \"name\": \"Cosine\",\n",
    "#                 \"learning_rate\": 0.001,\n",
    "#                 \"warmup_epoch\": 5\n",
    "#             }\n",
    "#         },\n",
    "#         \"Train\": {\n",
    "#             \"dataset\": {\n",
    "#                 \"name\": \"SimpleDataSet\",\n",
    "#                 \"data_dir\": \"dataset/\",\n",
    "#                 \"label_file_list\": [\"paddleocr_training/train_list.txt\"],\n",
    "#                 \"transforms\": [\n",
    "#                     {\"DecodeImage\": {\"img_mode\": \"RGB\", \"channel_first\": False}},\n",
    "#                     {\"RecResizeImg\": {\"image_shape\": [3, 32, 100]}},\n",
    "#                     {\"KeepKeys\": {\"keep_keys\": [\"image\", \"label\"]}}\n",
    "#                 ]\n",
    "#             },\n",
    "#             \"loader\": {\n",
    "#                 \"batch_size_per_card\": 32,\n",
    "#                 \"drop_last\": True,\n",
    "#                 \"num_workers\": 4,\n",
    "#                 \"shuffle\": True,\n",
    "#             }\n",
    "#         },\n",
    "#         \"Eval\": {\n",
    "#             \"dataset\": {\n",
    "#                 \"name\": \"SimpleDataSet\",\n",
    "#                 \"data_dir\": \"dataset/\",\n",
    "#                 \"label_file_list\": [\"paddleocr_training/val_list.txt\"],\n",
    "#                 \"transforms\": [\n",
    "#                     {\"DecodeImage\": {\"img_mode\": \"RGB\", \"channel_first\": False}},\n",
    "#                     {\"RecResizeImg\": {\"image_shape\": [3, 32, 100]}},\n",
    "#                     {\"KeepKeys\": {\"keep_keys\": [\"image\", \"label\"]}}\n",
    "#                 ]\n",
    "#             },\n",
    "#             \"loader\": {\n",
    "#                 \"batch_size_per_card\": 32,\n",
    "#                 \"drop_last\": False,\n",
    "#                 \"num_workers\": 4,\n",
    "#                 \"shuffle\": False,\n",
    "#             }\n",
    "#         },\n",
    "#         \"Global\": {\n",
    "#             \"epoch_num\": 100,\n",
    "#             \"save_model_dir\": \"./output/rec_crnn\",\n",
    "#             \"save_epoch_step\": 5,\n",
    "#             \"eval_batch_step\": [0, 2000],\n",
    "#             \"cal_metric_during_train\": True,\n",
    "#             \"pretrained_model\": \"./pretrain_models/en_PP-OCRv4_rec_train/best_accuracy\",\n",
    "#             \"checkpoints\": None,\n",
    "#             \"save_inference_dir\": None,\n",
    "#             \"use_visualdl\": True,\n",
    "#             \"class_num\": num_classes + 1,  # Add 1 for blank token\n",
    "#             \"character_dict_path\": \"paddleocr_training/dict.txt\",\n",
    "#             \"character_type\": \"ch\",\n",
    "#             \"use_space_char\": False,\n",
    "#             \"log_smooth_window\": 20,\n",
    "#             \"print_batch_step\": 10\n",
    "#         },\n",
    "#         \"PostProcess\": {\n",
    "#     \"name\": \"CTCLabelDecode\"  \n",
    "#         },\n",
    "#         \"Metric\": {\n",
    "#             \"name\": \"RecMetric\"\n",
    "#         }\n",
    "#     }\n",
    "    \n",
    "#     config_file = Path(output_path) / 'config.yml'\n",
    "#     with open(config_file, 'w') as f:\n",
    "#         json.dump(config, f, indent=2)\n",
    "    \n",
    "#     print(f\"Created configuration file at: {config_file}\")\n",
    "\n",
    "# def prepare_training():\n",
    "#     \"\"\"\n",
    "#     Prepare all necessary files for PaddleOCR training\n",
    "#     \"\"\"\n",
    "#     # Create output directory\n",
    "#     output_path = Path('paddleocr_training')\n",
    "#     output_path.mkdir(exist_ok=True)\n",
    "    \n",
    "#     # Create training lists\n",
    "#     for split in ['train', 'val', 'test']:\n",
    "#         create_training_list('dataset', output_path, split)\n",
    "    \n",
    "#     # Create dictionary\n",
    "#     num_classes = create_dict_file('dataset', output_path)\n",
    "    \n",
    "#     # Create configuration\n",
    "#     create_config(output_path, num_classes)\n",
    "    \n",
    "#     print(\"\\nTraining preparation completed!\")\n",
    "#     print(\"\\nTo start training, follow these steps:\")\n",
    "#     print(\"1. Install PaddlePaddle and PaddleOCR:\")\n",
    "#     print(\"   pip install paddlepaddle-gpu paddleocr\")\n",
    "#     print(\"\\n2. Run training:\")\n",
    "#     print(\"   python PaddleOCR/tools/train.py -c paddleocr_training/config.yml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024/12/11 16:58:05] ppocr WARNING: You are using VisualDL, the VisualDL is deprecated and removed in ppocr!\n",
      "[2024/12/11 16:58:05] ppocr INFO: Architecture : \n",
      "[2024/12/11 16:58:05] ppocr INFO:     Backbone : \n",
      "[2024/12/11 16:58:05] ppocr INFO:         layers : 34\n",
      "[2024/12/11 16:58:05] ppocr INFO:         name : ResNet\n",
      "[2024/12/11 16:58:05] ppocr INFO:     Head : \n",
      "[2024/12/11 16:58:05] ppocr INFO:         fc_decay : 0.0004\n",
      "[2024/12/11 16:58:05] ppocr INFO:         name : CTCHead\n",
      "[2024/12/11 16:58:05] ppocr INFO:     Neck : \n",
      "[2024/12/11 16:58:05] ppocr INFO:         encoder_type : rnn\n",
      "[2024/12/11 16:58:05] ppocr INFO:         hidden_size : 256\n",
      "[2024/12/11 16:58:05] ppocr INFO:         name : SequenceEncoder\n",
      "[2024/12/11 16:58:05] ppocr INFO:     Transform : None\n",
      "[2024/12/11 16:58:05] ppocr INFO:     algorithm : CRNN\n",
      "[2024/12/11 16:58:05] ppocr INFO:     model_type : rec\n",
      "[2024/12/11 16:58:05] ppocr INFO: Eval : \n",
      "[2024/12/11 16:58:05] ppocr INFO:     dataset : \n",
      "[2024/12/11 16:58:05] ppocr INFO:         data_dir : dataset/\n",
      "[2024/12/11 16:58:05] ppocr INFO:         label_file_list : ['val_list.txt']\n",
      "[2024/12/11 16:58:05] ppocr INFO:         name : SimpleDataSet\n",
      "[2024/12/11 16:58:05] ppocr INFO:         transforms : \n",
      "[2024/12/11 16:58:05] ppocr INFO:             DecodeImage : \n",
      "[2024/12/11 16:58:05] ppocr INFO:                 channel_first : False\n",
      "[2024/12/11 16:58:05] ppocr INFO:                 img_mode : RGB\n",
      "[2024/12/11 16:58:05] ppocr INFO:             RecResizeImg : \n",
      "[2024/12/11 16:58:05] ppocr INFO:                 image_shape : [3, 32, 100]\n",
      "[2024/12/11 16:58:05] ppocr INFO:             KeepKeys : \n",
      "[2024/12/11 16:58:05] ppocr INFO:                 keep_keys : ['image', 'label']\n",
      "[2024/12/11 16:58:05] ppocr INFO:     loader : \n",
      "[2024/12/11 16:58:05] ppocr INFO:         batch_size_per_card : 32\n",
      "[2024/12/11 16:58:05] ppocr INFO:         drop_last : False\n",
      "[2024/12/11 16:58:05] ppocr INFO:         num_workers : 4\n",
      "[2024/12/11 16:58:05] ppocr INFO: Global : \n",
      "[2024/12/11 16:58:05] ppocr INFO:     cal_metric_during_train : True\n",
      "[2024/12/11 16:58:05] ppocr INFO:     character_dict_path : dict.txt\n",
      "[2024/12/11 16:58:05] ppocr INFO:     character_type : ch\n",
      "[2024/12/11 16:58:05] ppocr INFO:     checkpoints : None\n",
      "[2024/12/11 16:58:05] ppocr INFO:     class_num : 38\n",
      "[2024/12/11 16:58:05] ppocr INFO:     distributed : False\n",
      "[2024/12/11 16:58:05] ppocr INFO:     epoch_num : 100\n",
      "[2024/12/11 16:58:05] ppocr INFO:     eval_batch_step : [0, 2000]\n",
      "[2024/12/11 16:58:05] ppocr INFO:     pretrained_model : None\n",
      "[2024/12/11 16:58:05] ppocr INFO:     save_epoch_step : 5\n",
      "[2024/12/11 16:58:05] ppocr INFO:     save_inference_dir : None\n",
      "[2024/12/11 16:58:05] ppocr INFO:     save_model_dir : ./output/rec_crnn\n",
      "[2024/12/11 16:58:05] ppocr INFO:     use_space_char : False\n",
      "[2024/12/11 16:58:05] ppocr INFO:     use_visualdl : True\n",
      "[2024/12/11 16:58:05] ppocr INFO: Loss : \n",
      "[2024/12/11 16:58:05] ppocr INFO:     name : CTCLoss\n",
      "[2024/12/11 16:58:05] ppocr INFO: Optimizer : \n",
      "[2024/12/11 16:58:05] ppocr INFO:     beta1 : 0.9\n",
      "[2024/12/11 16:58:05] ppocr INFO:     beta2 : 0.999\n",
      "[2024/12/11 16:58:05] ppocr INFO:     lr : \n",
      "[2024/12/11 16:58:05] ppocr INFO:         learning_rate : 0.001\n",
      "[2024/12/11 16:58:05] ppocr INFO:         name : Cosine\n",
      "[2024/12/11 16:58:05] ppocr INFO:         warmup_epoch : 5\n",
      "[2024/12/11 16:58:05] ppocr INFO:     name : Adam\n",
      "[2024/12/11 16:58:05] ppocr INFO: Train : \n",
      "[2024/12/11 16:58:05] ppocr INFO:     dataset : \n",
      "[2024/12/11 16:58:05] ppocr INFO:         data_dir : dataset/\n",
      "[2024/12/11 16:58:05] ppocr INFO:         label_file_list : ['train_list.txt']\n",
      "[2024/12/11 16:58:05] ppocr INFO:         name : SimpleDataSet\n",
      "[2024/12/11 16:58:05] ppocr INFO:         transforms : \n",
      "[2024/12/11 16:58:05] ppocr INFO:             DecodeImage : \n",
      "[2024/12/11 16:58:05] ppocr INFO:                 channel_first : False\n",
      "[2024/12/11 16:58:05] ppocr INFO:                 img_mode : RGB\n",
      "[2024/12/11 16:58:05] ppocr INFO:             RecResizeImg : \n",
      "[2024/12/11 16:58:05] ppocr INFO:                 image_shape : [3, 32, 100]\n",
      "[2024/12/11 16:58:05] ppocr INFO:             KeepKeys : \n",
      "[2024/12/11 16:58:05] ppocr INFO:                 keep_keys : ['image', 'label']\n",
      "[2024/12/11 16:58:05] ppocr INFO:     loader : \n",
      "[2024/12/11 16:58:05] ppocr INFO:         batch_size_per_card : 32\n",
      "[2024/12/11 16:58:05] ppocr INFO:         drop_last : True\n",
      "[2024/12/11 16:58:05] ppocr INFO:         num_workers : 4\n",
      "[2024/12/11 16:58:05] ppocr INFO: profiler_options : None\n",
      "[2024/12/11 16:58:05] ppocr INFO: train with paddle 2.6.2 and device Place(cpu)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"d:\\Project\\License_plate\\PaddleOCR\\tools\\train.py\", line 269, in <module>\n",
      "    main(config, device, logger, vdl_writer, seed)\n",
      "  File \"d:\\Project\\License_plate\\PaddleOCR\\tools\\train.py\", line 55, in main\n",
      "    train_dataloader = build_dataloader(config, \"Train\", device, logger, seed)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Project\\License_plate\\PaddleOCR\\ppocr\\data\\__init__.py\", line 107, in build_dataloader\n",
      "    dataset = eval(module_name)(config, mode, logger, seed)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Project\\License_plate\\PaddleOCR\\ppocr\\data\\simple_dataset.py\", line 46, in __init__\n",
      "    self.do_shuffle = loader_config[\"shuffle\"]\n",
      "                      ~~~~~~~~~~~~~^^^^^^^^^^^\n",
      "KeyError: 'shuffle'\n"
     ]
    }
   ],
   "source": [
    "!python PaddleOCR/tools/train.py -c paddleocr_training/config.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
